{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 80 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, October 24, by 11:59pm</p>\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Stochastic Gradient Descent (10pts)\n",
    "\n",
    "### 1:\n",
    "Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: $$ y = w_0 + w_1*x_1 + w_2*x_1^2 + w_3*x_2 $$ Hint: start from the cost function (Assume sum of squared error). If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$y_{pred} = w_0 + w_1*x_1 + w_2*(x_1)^2 + w_3*x_2$$\n",
    "\n",
    "$$E(w_0,w_1,w_2)=\\sum _{i=1}^{n}E_{i}(w_0,w_1,w_2)=\\sum _{i=1}^{n}\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)^{2}$$\n",
    "\n",
    "$$\\textbf{w}^{(\\tau + 1)}=\\textbf{w}^{(\\tau)}-\\eta \\nabla E_{i}(w_0,w_1,w_2) = \\textbf{w}^{(\\tau)}-\\eta \\nabla E_{i}(w_0,w_1,w_2)$$ \n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau + 1)} = \\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau)}-\\eta \\begin{bmatrix}\n",
    "  \\frac{\\partial}{\\partial w_0} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_1} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_2} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_3} E_{i}(w_0,w_1,w_2)\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_0} E_{i}(w_0,w_1,w_2) = \\frac{\\partial}{\\partial w_0}  \\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)^{2} = 2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_1} E_{i}(w_0,w_1,w_2) = \\frac{\\partial}{\\partial w_1}  \\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)^{2} = 2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)*x_{1,i} $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_2} E_{i}(w_0,w_1,w_2) = \\frac{\\partial}{\\partial w_2}  \\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)^{2} = 2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)*2*x_{1,i} $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_3} E_{i}(w_0,w_1,w_2) = \\frac{\\partial}{\\partial w_3}  \\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)^{2} = 2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right)*x_{2,i} $$\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau + 1)} = \\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau)}-\\eta \\begin{bmatrix}\n",
    "  \\frac{\\partial}{\\partial w_0} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_1} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_2} E_{i}(w_0,w_1,w_2)\\\\\n",
    "  \\frac{\\partial}{\\partial w_3} E_{i}(w_0,w_1,w_2)\n",
    " \\end{bmatrix} =  \\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau)} - \\eta*2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right) \\begin{bmatrix}\n",
    "  1\\\\\n",
    "  x_{1,i}\\\\\n",
    "  2x_{1,i}\\\\\n",
    "  x_{2,i}\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau + 1)} =  \\begin{bmatrix}\n",
    "  w_0\\\\\n",
    "  w_1\\\\\n",
    "  w_2\\\\\n",
    "  w_3\n",
    " \\end{bmatrix}^{(\\tau)} - \\eta*2*\\left(w_0 + w_1*x_{1,i} + w_2*(x_{1,i})^2 + w_3*x_{2,i} - y_{act}\\right) \\begin{bmatrix}\n",
    "  1\\\\\n",
    "  x_{1,i}\\\\\n",
    "  2x_{1,i}\\\\\n",
    "  x_{2,i}\n",
    " \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2: \n",
    "\n",
    "Write Python code for an SGD solution to the non-linear model $$ y = w_0 + w_1*x_1 + w_2*x_1^2 + w_3*x_2$$ Try to format similarly to scikit-learn's models. There should be a _fit_ function that takes parameters X, y, learning rate, and number of iterations, and a _predict_ function that takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 10 epochs, using learning rates: [.0001, .001, .01] . Plot MSE and the $w$ parameters as a function of epoch count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss_function_base(w0,w1,w2,w3,x1,x2,y_act):\n",
    "    return 2*(w0 + w1*x1 + w2*(x1**2) + w3*x2 - y_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def fit(self,X,y,learning_rate,number_iterations):\n",
    "        data = np.concatenate((X,y),1)      \n",
    "        x1_mat = X[:,0]\n",
    "        x2_mat = X[:,1]\n",
    "        self.x1 = x1_mat\n",
    "        self.x2 = x2_mat\n",
    "        self.y = y\n",
    "        w0=0\n",
    "        w1=0\n",
    "        w2=0\n",
    "        w3=0\n",
    "        sample_size = 20\n",
    "        for itter in range(number_iterations):\n",
    "            samplex1 = np.random.choice(data[:,0] , replace = False , size = sample_size)\n",
    "            samplex2 = np.random.choice(data[:,1] , replace = False , size = sample_size)\n",
    "            sampley = np.random.choice(data[:,2] , replace = False , size = sample_size)\n",
    "            for point in range(sample_size):\n",
    "                x1 = samplex1[point]\n",
    "                x2 = samplex2[point]\n",
    "                y_act = sampley[point]\n",
    "                w01, w11, w21, w31 = w0 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act), \\\n",
    "                                w1 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*x1,\\\n",
    "                                w2 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*2*x1,\\\n",
    "                                w3 - learning_rate*loss_function_base(w0,w1,w2,w3,x1,x2,y_act)*x2\n",
    "                w0 = w01\n",
    "                w1 = w11\n",
    "                w2 = w21\n",
    "                w3 = w31\n",
    "        self.w0=w0\n",
    "        self.w1=w1\n",
    "        self.w2=w2\n",
    "        self.w3=w3\n",
    "    \n",
    "    def predict(self,X):\n",
    "        x1_mat = X[:,0]\n",
    "        x2_mat = X[:,1]\n",
    "        y_pred = []\n",
    "        w0 = self.w0\n",
    "        w1 = self.w1\n",
    "        w2 = self.w2\n",
    "        w3 = self.w3\n",
    "        for point in range(len(x1_mat)):\n",
    "            x1 = x1_mat[point]\n",
    "            x2 = x2_mat[point]\n",
    "            y = self.w0 + ((w1)*(x1)) + w2*((x1)**2) + w3*x2\n",
    "            y_pred.append(y)\n",
    "        self.y_pred = y_pred\n",
    "        return y_pred\n",
    "    \n",
    "    def RMSE(self):\n",
    "        SE = 0\n",
    "        for i in range(len(self.y_pred)):\n",
    "            SE = SE + (self.y_pred[i]-self.y[:,0][i])**2\n",
    "        MSE = float(SE)/float(len(self.y_pred))\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_csv(\"samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_matrix = sample[[\"x1\",\"x2\",\"y\"]].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = GradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.fit(sample_matrix[:,0:2],sample_matrix[:,2:],0.0001,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = g.predict(sample_matrix[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.069036968965619"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.RMSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  [ 131.56073854]\n"
     ]
    }
   ],
   "source": [
    "Squared_error = 0\n",
    "for i in range(len(y)):\n",
    "    Squared_error = Squared_error + (y[i]-s_array[:,2:][i])**2\n",
    "print \"Mean Squared Error: \", Squared_error/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.83871802,  1.84596219,  2.41197613, -4.36592025, -4.17227961,\n",
       "        0.36661021, -1.05314224,  0.1465502 , -1.80132117, -2.63659113,\n",
       "        1.37053291, -6.90428813, -2.27509391, -3.75818917, -2.41051165,\n",
       "       -3.52808322,  1.06595109,  2.0489578 , -2.02691325, -2.85974649,\n",
       "        2.05371742, -5.62119715,  0.41548494, -6.70877661,  0.25836866,\n",
       "       -3.74228962, -3.94584841, -2.02992359, -3.76718292, -3.74834544,\n",
       "       -2.22726677,  0.25175204,  1.35761834,  1.22032662,  1.73715133,\n",
       "       -0.40335921,  0.31101897, -0.27996807, -1.53689914, -3.36557676])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_array[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Gradient Descent (5 pts)\n",
    "\n",
    "Suppose we are trying to use gradient descent to minimize a cost function y = f(w) as shown in the figure below. This function is linearly decreasing between A and B, constant between B and C, quadratic between C and D and constant between D and E. Assume that we have 10000 data points in our training set. If we choose the starting point between B and C, will we be able to find the local minima? Explain your answer. If your answer is \"Yes\", can you give a bound on the number of iterations required to get to the local minima?\n",
    "\n",
    "<img src=\"sgd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not find the local minima.\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "For some learning rate $\\eta$, the gradient descent procedes in a step wise fashion where $w^{(\\tau+1)} = w^{(\\tau)} + \\eta \\nabla f(w)$, where $\\tau \\in N$. \n",
    "\n",
    "Note that  $\\forall w_0,w_1$ where $B < w_0,w_1 < C$, the graph shows $f(w_0)=f(w_1)$. \n",
    "\n",
    "So it follows that $f(w_0)-f(w_1)=0$.\n",
    "\n",
    "This implies that $\\frac{f(w_0)-f(w_1)}{w_0-w_1}=\\frac{0}{w_0-w_1}=0.$ So $\\frac{f(w_0)-f(w_1)}{w_0-w_1}=0.$\n",
    "\n",
    "So $\\lim_{w_1 \\to w_0}\\frac{f(w_0)-f(w_1)}{w_0-w_1}=0$.\n",
    "\n",
    "By definition of derivative, it follows that $\\forall w$ where $B < w < C$, $\\frac{d f(w)}{d w}=0.$\n",
    "\n",
    "By definition of gradient, in this case $\\nabla f(w) = \\frac{d f(w)}{d w}=0$. So $\\nabla f(w)=0.$\n",
    "\n",
    "Thus the gradient descent equation becomes $w^{(\\tau+1)} = w^{(\\tau)} + \\eta \\nabla f(w)= w^{(\\tau)} + \\eta*0 = w^{(\\tau)}$.  \n",
    "\n",
    "So $\\forall \\tau$, $w^{(\\tau+1)} = w^{(\\tau)}$.\n",
    "\n",
    "---\n",
    "#### Lemma: \n",
    "\n",
    "If $\\forall \\tau \\in N$, $w^{(\\tau+1)} = w^{(\\tau)}$, then $\\forall \\epsilon, \\tau \\in N$,  $w^{(\\tau)} = w^{(\\epsilon)}$.\n",
    "\n",
    "Suppose not. That is, suppose $\\exists \\epsilon \\in N$ where $\\forall \\tau$, $w^{(\\tau)} \\neq w^{(\\epsilon)}$, $\\forall \\tau \\in N$.\n",
    "\n",
    "Note $\\exists \\tau \\in N$, where $\\epsilon - 1 = \\tau$ so $\\tau + 1 = \\epsilon$. So $w^{(\\tau)} \\neq w^{(\\tau + 1)}$. This is a contradiction.\n",
    "\n",
    "$\\therefore$ $\\forall \\tau \\in N$, $w^{(\\tau+1)} = w^{(\\tau)}$, then $\\forall \\epsilon, \\tau \\in N$,  $w^{(\\tau)} = w^{(\\epsilon)}$.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "$\\therefore$ For any starting point between A and B exclusive, every subsequent iteration will yield that same point. Since every point in that range is not itself the local minima, the itteration will never yield a point that will be the local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi-layer Perceptron regressor (15 points)\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the same dataset used in HW2 Q5: Hitters.csv [here](https://rdrr.io/cran/ISLR/man/Hitters.html). \n",
    "\n",
    "Following code will load and split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 42** and **test_size = 0.33**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-286adbcd342d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named model_selection"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('Hitters.csv')\n",
    "label_name = 'Salary'\n",
    "y = data[label_name]\n",
    "X = data.drop(label_name,axis=1)\n",
    "print X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=40, learning_rate_init = 0.001\n",
    "    \n",
    "Now, consider two different settings for the number of hidden units:\n",
    "    \n",
    "   (a) *hidden_layer_sizes = (2,)* (b) *hidden_layer_sizes = (15,)*\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model: (a) and (b) (6pts)\n",
    "   \n",
    "   \n",
    "2) Now, using the same parameters used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works better, (a) or (b)? Briefly analyze the result in terms of the number of hidden units. (5pts)\n",
    "\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each iteration. For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot two curves for model (a) and (b) in one figure, where *X-axis* is iteration number and *Y-axis* is squared root of *loss\\_curve\\_* value. (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Bayesian Classifiers (10 pts)\n",
    "\n",
    "Download the Smarket dataset from Canvas. This contains about four years worth of daily prices for one stock. The goal is to predict whether or not the stock price will go up or down, and the features are the stock prices of the last five days.  \n",
    "The code below loads the dataset and all necessary sklearn modules (not that you can't use more if you feel like it). Look up any module on the scikit-learn website for a full description.\n",
    "\n",
    "1. The last 50 points will be the test dataset. For training, use the 1000 points prior to these 50 test points.\n",
    "2. Train Linear Discriminant Analysis, Quadratic Discriminant Analysis, and (Gaussian) Naive Bayes. Extract the probability of the stock price going up for each row in the test set.\n",
    "3. Plot the receiver operating characteristic (ROC) curve of each model, using the extracted probabilities and the true values for the test set. (3 pts)\n",
    "4. Report the area under the ROC curve (AUC) for each model. (2 pts)\n",
    "6. Justify the performance of each model, relative to the others. (1 pts)\n",
    "7. Repeat steps 1-6, only using the prior 100 points for training. Explain the changes in model performance. (4 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "data = pd.read_csv('Smarket.csv', usecols=['Lag1','Lag2','Lag3','Lag4','Lag5','Direction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 - Logistic Regression (15pts)\n",
    "\n",
    "In this question we will be predicting mile per gallon (mpg) for Auto data set. ('Auto.csv' in Canvas)\n",
    "1. Convert mpg to a binary variable mpg01 which is 1 if had an mpg is greater than median mpg and zero otherwise\n",
    "2. Split the data into training and test. Use 42 as random seed and use 1/3rd of the data for testing. Our y variable is mpg01 and X matrix includes all the other variables except mpg01.\n",
    "3. Train a logistic regression with almost no regularization (pass l2 (ridge) to penalty and 1,000,000 to the C parameter which is the inverse of regularization strength lambda. This essentially does l2 regularization but applies very little weight to the penalty term) and report the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) on the test data. Also report the accuracy for the \"mpg01 = 0\" class, the \"mpg01 = 1\" class, and the average per-class accuracy on the test data. Average per-class accuracy is described in this [post](http://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/). You can use your confusion matrix to calculate this.\n",
    "4. Repeat step 3 except use l2 penalty with Cs of [0.001,0.01, 0.1, 1, 10 ,100, 1000]. You will want to use k-fold cross validation to select the best parameter. To evaluate which parameter is best, maximize the average per-class accuracy. To help with this task, check out [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and how to make your own [custom scorer](http://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "5. Repeat question 4 except use l1 instead of l2 as the penalty type, use Cs of  [0.001, 0.01, ..., 1000]\n",
    "6. Which model produces the best average per-class accuracy? Why do you think this is the case? How do the models handle the different classes, and why is this so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code will load and clean the dataset and load some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>307</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>318</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>304</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>302</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0   18          8           307         130    3504          12.0    70   \n",
       "1   15          8           350         165    3693          11.5    70   \n",
       "2   18          8           318         150    3436          11.0    70   \n",
       "3   16          8           304         150    3433          12.0    70   \n",
       "4   17          8           302         140    3449          10.5    70   \n",
       "\n",
       "   origin  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import cross_validation\n",
    "# from sklearn import model_selection # Use model_selection instead of cross_validation in sklearn version >=0.18\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "Auto = pd.read_csv('Auto.csv', na_values='?').drop('name',axis = 1).dropna()\n",
    "Auto.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 6: House Prices (kaggle competition) (25 pts)\n",
    "\n",
    "In this problem, we are going to explore a kaggle competition: [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Your goal is to obtain the best score you can in this competition. This is an ongoing competition, and you have the opportunity to win the prize money! \n",
    "\n",
    "The first step is to make a Kaggle account. Then find the House Prices competition and read the competition details and the description of the dataset. You may find this [article](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf) useful.\n",
    "\n",
    "Your work should meet the following requirements:\n",
    "\n",
    "1. Data Preprocessing. \n",
    " * Conduct some data preprocessing. (Hint: see if there is any skewed features and consider applying suitable transformation techniques to make them more \"normal\").\n",
    " * Impute the missing values (if any).\n",
    "2. Predictive Models. \n",
    " * You have to create at least three models: simple linear regression, Lasso and Ridge regression and multilayer perceptron. You may consider creating an ensemble of these models as well (optional). For Lasso and Ridge regression, optimize the alphas using cross validation. You may try other predictive models to get better scores (optional).\n",
    "3. Evaluation: submit your model to kaggle submission site and report the public score.\n",
    "\n",
    "Briefly describe your work on each of these steps. Explain (very briefly) what approaches you tried, what worked and what did not work. Mention your team's kaggle name and include a screen shot of your public submission score. Finally, try your best to win this competition!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
